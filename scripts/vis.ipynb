{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from train_utils import *\n",
    "\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECCONetwork(\n",
       "  (conv_fluid): EquiCtsConv2dRho1ToReg()\n",
       "  (conv_obstacle): EquiCtsConv2dRho1ToReg()\n",
       "  (dense_fluid): Sequential(\n",
       "    (0): EquiLinearRho1ToReg()\n",
       "    (1): EquiLinearRegToReg()\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): EquiCtsConv2dRegToReg()\n",
       "    (1): EquiCtsConv2dRegToReg()\n",
       "    (2): EquiCtsConv2dRegToReg()\n",
       "    (3): EquiCtsConv2dRegToRho1()\n",
       "  )\n",
       "  (denses): ModuleList(\n",
       "    (0): EquiLinearRegToReg()\n",
       "    (1): EquiLinearRegToReg()\n",
       "    (2): EquiLinearRegToReg()\n",
       "    (3): Sequential(\n",
       "      (0): EquiLinearRegToReg()\n",
       "      (1): EquiLinearRegToRho1()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'rho_reg_pecco'\n",
    "trained_model = torch.load('../checkpoints/' + model_name + '.pth',map_location=torch.device('cpu'))\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_network import evaluate\n",
    "from datasets.argoverse_lane_loader import read_pkl_data\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../argoverse_data/rose'\n",
    "val_path = os.path.join(dataset_path, 'val') #, 'lane_data'\n",
    "val_dataset = read_pkl_data(val_path, batch_size=1, shuffle=False, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating.. 1 {'nll': array(5.313828e+22, dtype=float32), 'ADE': 14718157000.0, 'ADE_std': 0.0, 'DE@1s': 2732.9297, 'DE@1s_std': 0.0, 'DE@2s': 43109724.0, 'DE@2s_std': 0.0, 'DE@3s': 275165700000.0, 'DE@3s_std': 0.0}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss, prediction_gt = evaluate(trained_model, val_dataset,\n",
    "                                       train_window=4, max_iter=1, \n",
    "                                       device='cpu', start_iter=0, \n",
    "                                        use_lane=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11221: (tensor([[ 2.1483e+03,  8.0175e+02],\n",
       "          [ 2.1486e+03,  8.0134e+02],\n",
       "          [ 2.1477e+03,  8.0232e+02],\n",
       "          [ 2.1481e+03,  8.0133e+02],\n",
       "          [ 2.1372e+03,  8.1296e+02],\n",
       "          [ 2.1366e+03,  8.0659e+02],\n",
       "          [ 2.0919e+03,  9.5330e+02],\n",
       "          [ 2.0984e+03,  8.1225e+02],\n",
       "          [ 1.9600e+03,  1.3754e+03],\n",
       "          [ 1.8579e+03, -3.9942e+01],\n",
       "          [ 1.9109e+03,  3.8494e+03],\n",
       "          [ 9.8147e+02, -6.6313e+03],\n",
       "          [ 4.0973e+03,  2.1403e+04],\n",
       "          [-6.9003e+03, -5.3963e+04],\n",
       "          [ 2.7576e+04,  1.4643e+05],\n",
       "          [-7.9871e+04, -3.8950e+05],\n",
       "          [ 2.4186e+05,  1.0380e+06],\n",
       "          [-6.5985e+05, -2.7135e+06],\n",
       "          [ 1.8721e+06,  7.1463e+06],\n",
       "          [-5.2430e+06, -1.8490e+07],\n",
       "          [ 1.5595e+07,  4.8216e+07],\n",
       "          [-4.4300e+07, -1.2490e+08],\n",
       "          [ 1.2761e+08,  3.2280e+08],\n",
       "          [-3.8341e+08, -8.4573e+08],\n",
       "          [ 1.2318e+09,  2.2189e+09],\n",
       "          [-3.6517e+09, -5.7720e+09],\n",
       "          [ 1.0526e+10,  1.4300e+10],\n",
       "          [-2.8949e+10, -3.5540e+10],\n",
       "          [ 8.0655e+10,  8.8592e+10],\n",
       "          [-2.2327e+11, -2.2375e+11]]),\n",
       "  tensor([[2149.0042,  800.9514],\n",
       "          [2149.0042,  800.9514],\n",
       "          [2149.4446,  800.4678],\n",
       "          [2149.8386,  799.9128],\n",
       "          [2150.0820,  799.5631],\n",
       "          [2150.9690,  798.5272],\n",
       "          [2151.3745,  797.9998],\n",
       "          [2151.7969,  797.5056],\n",
       "          [2152.8047,  796.3251],\n",
       "          [2153.2915,  795.7629],\n",
       "          [2153.6902,  795.2874],\n",
       "          [2154.1626,  794.8420],\n",
       "          [2155.0698,  793.7549],\n",
       "          [2155.3667,  793.4410],\n",
       "          [2155.9688,  792.4753],\n",
       "          [2156.8035,  791.7280],\n",
       "          [2157.2524,  791.1465],\n",
       "          [2157.8740,  790.4310],\n",
       "          [2158.3042,  789.8534],\n",
       "          [2159.3535,  788.5608],\n",
       "          [2159.8774,  787.9447],\n",
       "          [2160.5088,  787.1698],\n",
       "          [2161.0496,  786.5321],\n",
       "          [2161.6304,  785.7977],\n",
       "          [2162.7378,  784.4232],\n",
       "          [2163.3652,  783.7056],\n",
       "          [2163.9290,  783.0042],\n",
       "          [2164.4751,  782.2919],\n",
       "          [2164.9690,  781.6889],\n",
       "          [2165.5715,  780.8922]]))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 589.4445, 1381.8385]]]),\n",
       " tensor([[[ 589.6328, 1381.6088]]]),\n",
       " tensor([[[ 589.3992, 1380.2305]]]),\n",
       " tensor([[[ 588.9202, 1380.0366]]]),\n",
       " tensor([[[ 579.9214, 1367.5590]]]),\n",
       " tensor([[[ 584.3655, 1377.3729]]]),\n",
       " tensor([[[ 596.6658, 1271.8363]]]),\n",
       " tensor([[[ 640.7068, 1356.7920]]]),\n",
       " tensor([[[597.8172, 894.6785]]]),\n",
       " tensor([[[ 828.4139, 1853.6736]]]),\n",
       " tensor([[[  226.0106, -1018.5403]]]),\n",
       " tensor([[[1994.9950, 6174.8740]]]),\n",
       " tensor([[[ -2903.6455, -13136.2568]]]),\n",
       " tensor([[[10917.8076, 36676.0664]]]),\n",
       " tensor([[[-29450.9258, -93312.3594]]]),\n",
       " tensor([[[ 86699.4688, 243709.0000]]]),\n",
       " tensor([[[-246646.8906, -628298.2500]]]),\n",
       " tensor([[[ 739073.2500, 1645480.5000]]]),\n",
       " tensor([[[-2385644.5000, -4320317.0000]]]),\n",
       " tensor([[[ 7071482., 11239530.]]]),\n",
       " tensor([[[-20403056., -27931576.]]])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 587.6106, 1381.2997]]]),\n",
       " tensor([[[ 587.6106, 1381.2997]]]),\n",
       " tensor([[[ 586.9632, 1380.9113]]]),\n",
       " tensor([[[ 586.6739, 1380.7225]]]),\n",
       " tensor([[[ 586.3462, 1380.4492]]]),\n",
       " tensor([[[ 585.7589, 1379.9354]]]),\n",
       " tensor([[[ 585.4659, 1379.6121]]]),\n",
       " tensor([[[ 584.9191, 1379.0210]]]),\n",
       " tensor([[[ 584.6808, 1378.6652]]]),\n",
       " tensor([[[ 584.3546, 1378.2303]]]),\n",
       " tensor([[[ 583.8191, 1377.5601]]]),\n",
       " tensor([[[ 583.6369, 1377.1196]]]),\n",
       " tensor([[[ 583.3925, 1376.6952]]]),\n",
       " tensor([[[ 582.9846, 1375.8479]]]),\n",
       " tensor([[[ 582.7881, 1375.3280]]]),\n",
       " tensor([[[ 582.6030, 1374.8840]]]),\n",
       " tensor([[[ 582.4222, 1374.3217]]]),\n",
       " tensor([[[ 582.2029, 1373.8376]]]),\n",
       " tensor([[[ 582.0811, 1373.3918]]]),\n",
       " tensor([[[ 581.8999, 1372.7820]]]),\n",
       " tensor([[[ 581.7318, 1372.3556]]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gt.keys()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
